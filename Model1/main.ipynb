{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T07:00:35.956748Z",
     "start_time": "2024-09-01T07:00:09.944560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ROOT = os.getcwd().split('\\\\LLM')[0] + '\\\\LLM\\\\data\\\\'\n",
    "supplier_df = pd.read_excel(os.path.join(ROOT, \"1109c6390984f0870f18136529af3830.xlsx\"))\n",
    "category_df = pd.read_excel(os.path.join(ROOT, \"Категориия OZON.xlsx\"))\n",
    "mapping_df = pd.read_excel(os.path.join(ROOT, \"OZON.xlsx\"))\n",
    "\n",
    "mapping_df = mapping_df[['Артикул', 'type_id', 'description_category_id']]\n",
    "mapping_df.drop_duplicates(inplace=True)\n",
    "\n",
    "vcdf = mapping_df['Артикул'].value_counts()\n",
    "vcdf = vcdf[vcdf==1]\n",
    "articuls = vcdf.index.tolist()\n",
    "\n",
    "clean_mapping_df = mapping_df[mapping_df['Артикул'].isin(articuls)]\n",
    "\n",
    "clean_supplier_df = supplier_df[supplier_df['Код артикула'].isin(clean_mapping_df['Артикул'])]\n",
    "clean_supplier_df.loc[:, 'text'] = clean_supplier_df.apply(lambda row: '/'.join(row[['Название', 'Группа товаров', 'Раздел']].tolist()), axis=1)\n",
    "clean_supplier_df.rename(columns={'Код артикула':'articul'}, inplace=True)\n",
    "\n",
    "prepare_supplier_df = clean_supplier_df[['articul', 'text']]\n",
    "if prepare_supplier_df.shape[0] != clean_supplier_df.shape[0]:\n",
    "  print('ERROR: не все товары были найдены у поставщика')\n",
    "\n",
    "def get_category_text(v) -> str | None:\n",
    "    type_id = v['type_id']\n",
    "    description_category_id = v['description_category_id']\n",
    "    filtered_category_df = category_df[((category_df['2_type_id'] == type_id) & (category_df['1_description_category_id'] == description_category_id))]\n",
    "    if filtered_category_df.empty:\n",
    "      print(f'NOT FOUND ERROR: {type_id}')\n",
    "      return None\n",
    "    elif filtered_category_df.shape[0] > 1:\n",
    "      print(f'NOT UNIQUE ERROR: {type_id}')\n",
    "    else:\n",
    "      return '/'.join(filtered_category_df.iloc[0][['0_category_name', '1_category_name', '2_type_name']].tolist())\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'text': clean_mapping_df.merge(right=prepare_supplier_df, left_on='Артикул', right_on='articul', how='left')['text'].tolist(),\n",
    "    'label': clean_mapping_df[['description_category_id', 'type_id']].apply(get_category_text, axis=1).tolist()\n",
    "})\n",
    "\n"
   ],
   "id": "f313f0aa3d9bf00a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:27:31.704169Z",
     "start_time": "2024-09-01T08:27:31.281079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "supplier_df.to_pickle(os.path.join(ROOT, \"supplier_df.pkl\"))\n",
    "category_df.to_pickle(os.path.join(ROOT, \"category_df.pkl\"))\n",
    "mapping_df.to_pickle(os.path.join(ROOT, \"mapping_df.pkl\"))"
   ],
   "id": "4456c341e10335f0",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:20:00.138407Z",
     "start_time": "2024-09-01T08:20:00.123408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = test_data.iloc[:500, :]\n",
    "df['label_1'] = df['label'].apply(lambda row: row.split('/')[0])\n",
    "df['label_2'] = df['label'].apply(lambda row: row.split('/')[1])\n",
    "df['label_3'] = df['label'].apply(lambda row: row.split('/')[2])"
   ],
   "id": "5038b325453f1829",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pbar = None\n",
    "        \n",
    "    def text_cleaner(self, text) -> str:\n",
    "        text = re.sub(r'[^а-яА-Я/]+', '', text)\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        if self.pbar:\n",
    "            self.pbar.update(1)\n",
    "        return text\n",
    "    \n",
    "    def tokenizer(self, texts: list, max_length:int = 512) -> dict:\n",
    "        result = {'text_token': list(),\n",
    "                  'token_weight': list()}\n",
    "        for text in texts:\n",
    "            text_token = [ord(t) for t in text]\n",
    "            token_weight = [1] * len(text_token)\n",
    "            token_weight += [0] * (max_length - len(text_token))\n",
    "            text_token += [0] * (max_length - len(text_token))\n",
    "            result['text_token'].append(text_token)\n",
    "            result['token_weight'].append(token_weight)\n",
    "            if self.pbar:\n",
    "                self.pbar.update(1)\n",
    "        return result\n",
    "    \n",
    "    def prepare(self, texts: list, show=True):\n",
    "        if show:\n",
    "            self.pbar = tqdm(total=len(texts), desc='CLEAN')\n",
    "        else:\n",
    "            self.pbar = None\n",
    "        text = [self.text_cleaner(t) for t in texts]\n",
    "        \n",
    "        if show:\n",
    "            self.pbar = tqdm(total=len(texts), desc='TOKENIZE')\n",
    "        else:\n",
    "            self.pbar = None\n",
    "        tokens = self.tokenizer(text)\n",
    "\n",
    "        return tokens \n",
    "\n",
    "input_data = CustomTokenizer().prepare(texts=data['text'].tolist(), show=True)\n",
    "\n",
    "maper = {label: idx for idx, label in enumerate(data.label.unique())}\n"
   ],
   "id": "edb273ff0ff82e8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:20:01.516529Z",
     "start_time": "2024-09-01T08:20:00.971983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Подготовка данных\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Преобразуем категории в числовые метки\n",
    "def my_mapper(x):\n",
    "    if x == \"Строительство и ремонт\":\n",
    "        return 1\n",
    "    elif x == \"Дом и сад\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "label_mapping = {label: my_mapper(label) for label in df['label_1'].unique()}\n",
    "df['label_id'] = df['label_1'].map(label_mapping)\n",
    "\n",
    "# 2. Подготовка данных для тренировки\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'].tolist(),\n",
    "                                                                    df['label_id'].tolist(),\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Создание тензоров датасета\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_labels)\n",
    "val_dataset = Dataset(val_encodings, val_labels)"
   ],
   "id": "f9c99dde6da145f9",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:55:58.101091200Z",
     "start_time": "2024-09-01T08:55:45.756085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Модель и тренировка\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    disable_tqdm=False,  # Включаем отображение прогресса в консоли\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Тестирование на новых данных\n",
    "test_texts = test_data.iloc[:5, 0].tolist()\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Предсказание\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Выводим результаты\n",
    "for text, pred in zip(test_texts, predictions):\n",
    "    print(f\"Товар: {text} -> Предсказанная категория: {list(label_mapping.keys())[pred]}\")\n",
    "print()\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Путь к директории с сохраненной моделью\n",
    "model_dir = './results'\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Пример текста для предсказания\n",
    "text = \"Пример текста для классификации.\"\n",
    "\n",
    "# Токенизация текста\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Получение предсказаний\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Предсказание метки\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "print(f\"Предсказанная метка: {prediction.item()}\")\n"
   ],
   "id": "8f1db10706fbfc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/600 00:08 < 16:21, 0.60 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:26:08.303018Z",
     "start_time": "2024-09-01T09:26:08.049835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Путь к директории с сохраненной моделью\n",
    "model_dir = './model_1'\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")"
   ],
   "id": "84b211d52ccd6ff1",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:26:09.106759Z",
     "start_time": "2024-09-01T09:26:09.096758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = test_data.iloc[0, 0]\n",
    "text, len(text)"
   ],
   "id": "8fa763f6a753e9d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Очки защитные закрытого типа с непрямой вентиляцией, поликарбонат Россия Сибртех/Ручной инструмент/Отделочный инструмент/Средства индивидуальной защиты/Очки защитные',\n",
       " 165)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:36:55.866357Z",
     "start_time": "2024-09-01T09:36:55.844366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_mapper(x):\n",
    "    if x == \"Строительство и ремонт\":\n",
    "        return 1\n",
    "    elif x == \"Дом и сад\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "df = test_data[['text', 'label_1']]\n",
    "df['label_id'] = df['label_1'].apply(my_mapper)\n",
    "df['label_id'].value_counts()"
   ],
   "id": "2842aaa1c5d31db1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "1    2540\n",
       "2     408\n",
       "0      62\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:45:54.399399Z",
     "start_time": "2024-09-01T09:37:03.598658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pbar = None\n",
    "def get_preds(text):\n",
    "    global pbar\n",
    "    # Токенизация текста\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Получение предсказаний\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "    if pbar:\n",
    "        pbar.update(1)\n",
    "    return prediction.item()\n",
    "\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=len(df))\n",
    "df['preds'] = df['text'].apply(get_preds) \n"
   ],
   "id": "6f34ef912357c03e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]\n",
      "100%|██████████| 3010/3010 [08:50<00:00,  5.99it/s]"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:46:17.096925Z",
     "start_time": "2024-09-01T09:46:17.084443Z"
    }
   },
   "cell_type": "code",
   "source": "sum(df['label_id'] == df['preds'])/len(df)*100",
   "id": "e398b7792076dd1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.38538205980066"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:46:20.536017Z",
     "start_time": "2024-09-01T09:46:20.523468Z"
    }
   },
   "cell_type": "code",
   "source": "df['preds'].value_counts()",
   "id": "11ce9d51d43634e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preds\n",
       "1    3010\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:46:26.871689Z",
     "start_time": "2024-09-01T09:46:26.856668Z"
    }
   },
   "cell_type": "code",
   "source": "logits",
   "id": "7521b4140e12b6cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.2273,   8.4003, -15.9908]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T09:28:34.981926Z",
     "start_time": "2024-09-01T09:28:34.973925Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eedeca64e5f13732",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
